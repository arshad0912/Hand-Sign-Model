{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rfdzkxyxtv64",
        "outputId": "32dcf142-a69c-44f3-9783-a0f98c783d27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n",
            "/bin/bash: line 1: conda: command not found\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "INFO: pip is looking at multiple versions of mediapipe to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mediapipe-0.10.20-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "  Downloading mediapipe-0.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "  Downloading mediapipe-0.10.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "  Downloading mediapipe-0.10.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading mediapipe-0.10.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.14 protobuf-4.25.8 sounddevice-0.5.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "fe47e73394444cd8957c0b67773b6cb3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# create a clean env (conda or venv)\n",
        "!conda create -n handsign python=3.11 -y\n",
        "!conda activate handsign\n",
        "\n",
        "!pip install opencv-python mediapipe numpy pandas scikit-learn tensorflow matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read video\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzSckFbxuDA3",
        "outputId": "47fc0bcc-f1fc-493f-98a6-b34bcf34b33b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If MoviePy isn't installed, uncomment the next line:\n",
        "!pip install moviepy\n",
        "\n",
        "import moviepy.editor as mp\n",
        "from IPython.display import display\n",
        "\n",
        "# Load the video file\n",
        "video_clip = mp.VideoFileClip(\"/content/drive/MyDrive/videoplayback-2.mp4\")\n",
        "\n",
        "# Display the video\n",
        "display(video_clip)"
      ],
      "metadata": {
        "id": "iOskyYUsu9jm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "fbf55f0d-947a-4c52-9b10-cb70badb0d24"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.0.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.8.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  match = re.search('\\d+$', rotation_line)\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<moviepy.video.io.VideoFileClip.VideoFileClip at 0x7e433582afc0>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# video_path,start,end,label\n",
        "# /content/drive/MyDrive/videoplayback-2,00:00:17.2,00:00:19.6,HELLO\n",
        "# /content/drive/MyDrive/videoplayback-2,00:00:24.0,00:00:26.3,SEE_YOU_LATER\n",
        "# /content/drive/MyDrive/videoplayback-2,00:01:36.0,00:01:39.3,I OR ME\n",
        "# /content/drive/MyDrive/videoplayback-2,00:01:46.0,00:01:47.3,FATHER\n",
        "# /content/drive/MyDrive/videoplayback-2,00:01:55.0,00:01:57.3,MOTHER\n",
        "# /content/drive/MyDrive/videoplayback-2,00:02:02.0,00:02:05.3,YES\n",
        "# /content/drive/MyDrive/videoplayback-2,00:02:11.0,00:02:13.3,NO\n",
        "# /content/drive/MyDrive/videoplayback-2,00:02:20.0,00:02:22.3,HELP\n",
        "# /content/drive/MyDrive/videoplayback-2,00:02:33.0,00:02:35.3,PLEASE\n",
        "# /content/drive/MyDrive/videoplayback-2,00:02:41.0,00:02:43.3,THANK YOU\n",
        "# /content/drive/MyDrive/videoplayback-2,00:02:50.0,00:02:51.3,WANT\n",
        "# /content/drive/MyDrive/videoplayback-2,00:03:00.0,00:03:01.3,WHAT\n",
        "# /content/drive/MyDrive/videoplayback-2,00:03:07.0,00:03:09.3,DOG\n",
        "# /content/drive/MyDrive/videoplayback-2,00:03:15.0,00:03:17.3,CAT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "5wxJxkO1y1hr",
        "outputId": "ab75521b-2fac-42fa-a9cd-65a26b26c80e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "only single target (not tuple) can be annotated (ipython-input-1244559417.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1244559417.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    content/drive/MyDrive/videoplayback-2,00:00:17.2,00:00:19.6,HELLO()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m only single target (not tuple) can be annotated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/labels.csv\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj4SsgE41NX8",
        "outputId": "94c762c3-0f31-4a50-8531-9f9c9affa765"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    video_path       start         end  \\\n",
            "0   /content/drive/MyDrive/videoplayback-2.mp4  00:00:17.2  00:00:19.6   \n",
            "1   /content/drive/MyDrive/videoplayback-2.mp4  00:00:24.0  00:00:26.3   \n",
            "2   /content/drive/MyDrive/videoplayback-2.mp4  00:01:36.0  00:01:39.3   \n",
            "3   /content/drive/MyDrive/videoplayback-2.mp4  00:01:46.0  00:01:47.3   \n",
            "4   /content/drive/MyDrive/videoplayback-2.mp4  00:01:55.0  00:01:57.3   \n",
            "5   /content/drive/MyDrive/videoplayback-2.mp4  00:02:02.0  00:02:05.3   \n",
            "6   /content/drive/MyDrive/videoplayback-2.mp4  00:02:11.0  00:02:13.3   \n",
            "7   /content/drive/MyDrive/videoplayback-2.mp4  00:02:20.0  00:02:22.3   \n",
            "8   /content/drive/MyDrive/videoplayback-2.mp4  00:02:33.0  00:02:35.3   \n",
            "9   /content/drive/MyDrive/videoplayback-2.mp4  00:02:41.0  00:02:43.3   \n",
            "10  /content/drive/MyDrive/videoplayback-2.mp4  00:02:50.0  00:02:51.3   \n",
            "11  /content/drive/MyDrive/videoplayback-2.mp4  00:03:00.0  00:03:01.3   \n",
            "12  /content/drive/MyDrive/videoplayback-2.mp4  00:03:07.0  00:03:09.3   \n",
            "13  /content/drive/MyDrive/videoplayback-2.mp4  00:03:15.0  00:03:17.3   \n",
            "\n",
            "            label  \n",
            "0           HELLO  \n",
            "1   SEE_YOU_LATER  \n",
            "2         I OR ME  \n",
            "3          FATHER  \n",
            "4          MOTHER  \n",
            "5             YES  \n",
            "6              NO  \n",
            "7            HELP  \n",
            "8          PLEASE  \n",
            "9       THANK YOU  \n",
            "10           WANT  \n",
            "11           WHAT  \n",
            "12            DOG  \n",
            "13            CAT  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make_dataset.py\n",
        "import os, math, cv2, mediapipe as mp, numpy as np, pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "DATA_DIR = \"data\"                     # will create data/sequences and data/meta.csv\n",
        "SEQ_LEN = 30                          # frames per example\n",
        "os.makedirs(os.path.join(DATA_DIR, \"sequences\"), exist_ok=True)\n",
        "\n",
        "def ts_to_sec(ts):\n",
        "    # \"HH:MM:SS.mmm\" -> seconds (float)\n",
        "    dt = datetime.strptime(ts, \"%H:%M:%S.%f\") if \".\" in ts else datetime.strptime(ts, \"%H:%M:%S\")\n",
        "    return dt.hour*3600 + dt.minute*60 + dt.second + (dt.microsecond/1e6)\n",
        "\n",
        "def read_segments(csv_path=\"/content/drive/MyDrive/labels.csv\"):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # normalize times to seconds\n",
        "    df[\"start_s\"] = df[\"start\"].apply(ts_to_sec)\n",
        "    df[\"end_s\"]   = df[\"end\"].apply(ts_to_sec)\n",
        "    return df\n",
        "\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, model_complexity=1,\n",
        "                       min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "\n",
        "def get_landmarks_both_hands(frame_bgr):\n",
        "    # returns (126,) vector = 2 hands * (21*3) ; missing hand => zeros\n",
        "    image_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "    res = hands.process(image_rgb)\n",
        "    # Collect up to two hands, each 63 float: (x,y,z) for 21 landmarks\n",
        "    hands_vecs = []\n",
        "    if res.multi_hand_landmarks:\n",
        "        # sort by handedness (optional): right first then left for consistency\n",
        "        order = []\n",
        "        for i, handed in enumerate(res.multi_handedness):\n",
        "            label = handed.classification[0].label.upper()  # \"Left\"/\"Right\"\n",
        "            order.append((0 if label==\"RIGHT\" else 1, i))\n",
        "        order.sort()\n",
        "        for _, idx in order[:2]:\n",
        "            lm = res.multi_hand_landmarks[idx]\n",
        "            vec = []\n",
        "            for p in lm.landmark:\n",
        "                vec.extend([p.x, p.y, p.z])\n",
        "            hands_vecs.append(np.array(vec, dtype=np.float32))\n",
        "    # pad to 2 hands\n",
        "    while len(hands_vecs) < 2:\n",
        "        hands_vecs.append(np.zeros(63, dtype=np.float32))\n",
        "    return np.concatenate(hands_vecs)  # (126,)\n",
        "\n",
        "def sample_frames(cap, start_s, end_s, n=SEQ_LEN):\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    total = max(end_s - start_s, 1e-3)\n",
        "    times = np.linspace(start_s, end_s, n, endpoint=False)\n",
        "    frames = []\n",
        "    for t in times:\n",
        "        cap.set(cv2.CAP_PROP_POS_MSEC, t*1000)\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "    return frames\n",
        "\n",
        "def main():\n",
        "    df = read_segments()\n",
        "    rows = []\n",
        "    for vid_path in df[\"video_path\"].unique():\n",
        "        if not os.path.isfile(vid_path):\n",
        "            print(f\"Missing video: {vid_path}\")\n",
        "    for i, row in df.iterrows():\n",
        "        vid, start_s, end_s, label = row[\"video_path\"], row[\"start_s\"], row[\"end_s\"], row[\"label\"]\n",
        "        cap = cv2.VideoCapture(vid)\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Could not open {vid}\")\n",
        "            continue\n",
        "        frames = sample_frames(cap, start_s, end_s, SEQ_LEN)\n",
        "        cap.release()\n",
        "        if len(frames) < SEQ_LEN//2:\n",
        "            print(f\"Too few frames for {label} [{start_s}-{end_s}]\")\n",
        "            continue\n",
        "\n",
        "        seq = []\n",
        "        for f in frames:\n",
        "            vec = get_landmarks_both_hands(f)   # (126,)\n",
        "            seq.append(vec)\n",
        "        seq = np.stack(seq, axis=0)             # (T, 126)\n",
        "\n",
        "        # optional normalization: center by wrist (landmark 0 of right-hand slot)\n",
        "        # Here we skip—MediaPipe already gives normalized x,y (0..1). z is relative.\n",
        "\n",
        "        out_name = f\"{label}_{i:05d}.npy\"\n",
        "        out_path = os.path.join(DATA_DIR, \"sequences\", out_name)\n",
        "        np.save(out_path, seq)\n",
        "        rows.append({\"path\": out_path, \"label\": label, \"nframes\": len(frames)})\n",
        "        print(\"Saved\", out_path)\n",
        "\n",
        "    pd.DataFrame(rows).to_csv(os.path.join(DATA_DIR, \"meta.csv\"), index=False)\n",
        "    print(\"Wrote meta.csv with\", len(rows), \"rows\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pkLzA4T1ngm",
        "outputId": "22229c2a-5a63-49ac-b46c-cc440122da3f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved data/sequences/HELLO_00000.npy\n",
            "Saved data/sequences/SEE_YOU_LATER_00001.npy\n",
            "Saved data/sequences/I OR ME_00002.npy\n",
            "Saved data/sequences/FATHER_00003.npy\n",
            "Saved data/sequences/MOTHER_00004.npy\n",
            "Saved data/sequences/YES_00005.npy\n",
            "Saved data/sequences/NO_00006.npy\n",
            "Saved data/sequences/HELP_00007.npy\n",
            "Saved data/sequences/PLEASE_00008.npy\n",
            "Saved data/sequences/THANK YOU_00009.npy\n",
            "Saved data/sequences/WANT_00010.npy\n",
            "Saved data/sequences/WHAT_00011.npy\n",
            "Saved data/sequences/DOG_00012.npy\n",
            "Saved data/sequences/CAT_00013.npy\n",
            "Wrote meta.csv with 14 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 — create the script file\n",
        "%%writefile make_dataset.py\n",
        "# (paste the entire make_dataset.py content here)\n",
        "# tip: if you already have it above in the chat, paste it between these lines\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYsWKHzM8EJa",
        "outputId": "aafd29f9-b875-475a-eb04-462cfe8f1284"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing make_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python make_dataset.py"
      ],
      "metadata": {
        "id": "qzkeJiH_2X9S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== train.py (DROP-IN REPLACEMENT) =====\n",
        "import os, json, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "META = \"data/meta.csv\"\n",
        "SEQ_LEN = 30\n",
        "FEATS = 126\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 32\n",
        "TARGET_MIN_PER_CLASS = 4   # oversample each class up to at least this many rows\n",
        "\n",
        "# ---------- 1) LOAD META & CLEAN LABELS ----------\n",
        "if not os.path.isfile(META):\n",
        "    raise FileNotFoundError(f\"Can't find {META}. Run make_dataset.py first.\")\n",
        "\n",
        "df = pd.read_csv(META)\n",
        "\n",
        "# clean/sanitize labels (optional but handy)\n",
        "df[\"label\"] = (\n",
        "    df[\"label\"]\n",
        "    .astype(str)\n",
        "    .str.strip()\n",
        "    .str.replace(r\"\\s+\", \"_\", regex=True)\n",
        ")\n",
        "\n",
        "print(\"== RAW per-class counts ==\")\n",
        "print(df[\"label\"].value_counts(), \"\\n\")\n",
        "\n",
        "# ---------- 2) OVERSAMPLE SMALL CLASSES ----------\n",
        "# This prevents stratified split from failing when classes have only 1 sample.\n",
        "parts = []\n",
        "rng = np.random.RandomState(42)\n",
        "for lab, dfg in df.groupby(\"label\", sort=False):\n",
        "    need = max(0, TARGET_MIN_PER_CLASS - len(dfg))\n",
        "    if need > 0:\n",
        "        # sample with replacement to reach target min\n",
        "        gain = dfg.sample(n=need, replace=True, random_state=42)\n",
        "        parts.append(pd.concat([dfg, gain], ignore_index=True))\n",
        "    else:\n",
        "        parts.append(dfg)\n",
        "df = pd.concat(parts, ignore_index=True).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"== AFTER OVERSAMPLE per-class counts ==\")\n",
        "print(df[\"label\"].value_counts(), \"\\n\")\n",
        "\n",
        "# ---------- 3) LOAD ALL SEQUENCES ----------\n",
        "paths = df[\"path\"].tolist()\n",
        "labels = df[\"label\"].tolist()\n",
        "\n",
        "X = np.zeros((len(paths), SEQ_LEN, FEATS), dtype=np.float32)\n",
        "for i, p in enumerate(paths):\n",
        "    if not os.path.isfile(p):\n",
        "        raise FileNotFoundError(f\"Sequence file missing: {p}\\nDid make_dataset.py run correctly?\")\n",
        "    X[i] = np.load(p)\n",
        "\n",
        "# ---------- 4) PREPARE LABELS ----------\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(labels)\n",
        "num_classes = len(le.classes_)\n",
        "print(f\"Found {num_classes} classes: {list(le.classes_)}\\n\")\n",
        "\n",
        "# Save label mapping\n",
        "label_map = {i: label for i, label in enumerate(le.classes_)}\n",
        "with open(\"label_map.json\", \"w\") as f:\n",
        "    json.dump(label_map, f)\n",
        "\n",
        "# ---------- 5) SPLIT DATA ----------\n",
        "# Explicitly set test_size to number of classes to ensure at least one sample per class\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=num_classes, train_size=len(y) - num_classes, stratify=y, random_state=42)\n",
        "\n",
        "print(f\"Train shapes: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"Test shapes:  X={X_test.shape}, y={y_test.shape}\\n\")\n",
        "\n",
        "# ---------- 6) BUILD MODEL ----------\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(SEQ_LEN, FEATS)),\n",
        "    layers.LSTM(64, return_sequences=True),\n",
        "    layers.LSTM(128),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# ---------- 7) TRAIN MODEL ----------\n",
        "print(\"\\nTraining model...\")\n",
        "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "                    validation_split=0.2, verbose=1)\n",
        "\n",
        "# ---------- 8) EVALUATE MODEL ----------\n",
        "print(\"\\nEvaluating model...\")\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# ---------- 9) SAVE MODEL (optional) ----------\n",
        "model.save(\"hand_sign_model.h5\")\n",
        "print(\"\\nModel saved to hand_sign_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OMoTHkMn88YB",
        "outputId": "eafb0086-9c7f-4fa6-a391-249a2af3dee3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== RAW per-class counts ==\n",
            "label\n",
            "HELLO            1\n",
            "SEE_YOU_LATER    1\n",
            "I_OR_ME          1\n",
            "FATHER           1\n",
            "MOTHER           1\n",
            "YES              1\n",
            "NO               1\n",
            "HELP             1\n",
            "PLEASE           1\n",
            "THANK_YOU        1\n",
            "WANT             1\n",
            "WHAT             1\n",
            "DOG              1\n",
            "CAT              1\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "== AFTER OVERSAMPLE per-class counts ==\n",
            "label\n",
            "HELLO            4\n",
            "SEE_YOU_LATER    4\n",
            "PLEASE           4\n",
            "FATHER           4\n",
            "MOTHER           4\n",
            "DOG              4\n",
            "THANK_YOU        4\n",
            "NO               4\n",
            "WHAT             4\n",
            "CAT              4\n",
            "HELP             4\n",
            "I_OR_ME          4\n",
            "WANT             4\n",
            "YES              4\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Found 14 classes: [np.str_('CAT'), np.str_('DOG'), np.str_('FATHER'), np.str_('HELLO'), np.str_('HELP'), np.str_('I_OR_ME'), np.str_('MOTHER'), np.str_('NO'), np.str_('PLEASE'), np.str_('SEE_YOU_LATER'), np.str_('THANK_YOU'), np.str_('WANT'), np.str_('WHAT'), np.str_('YES')]\n",
            "\n",
            "Train shapes: X=(42, 30, 126), y=(42,)\n",
            "Test shapes:  X=(14, 30, 126), y=(14,)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m48,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,816\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)             │           \u001b[38;5;34m910\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">48,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m156,878\u001b[0m (612.80 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">156,878</span> (612.80 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m156,878\u001b[0m (612.80 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">156,878</span> (612.80 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model...\n",
            "Epoch 1/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 466ms/step - accuracy: 0.0000e+00 - loss: 2.6490 - val_accuracy: 0.2222 - val_loss: 2.6204\n",
            "Epoch 2/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.1225 - loss: 2.6008 - val_accuracy: 0.1111 - val_loss: 2.6031\n",
            "Epoch 3/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.0612 - loss: 2.5768 - val_accuracy: 0.1111 - val_loss: 2.5969\n",
            "Epoch 4/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.1531 - loss: 2.5463 - val_accuracy: 0.2222 - val_loss: 2.6059\n",
            "Epoch 5/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.2039 - loss: 2.5211 - val_accuracy: 0.1111 - val_loss: 2.6278\n",
            "Epoch 6/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.1531 - loss: 2.4878 - val_accuracy: 0.1111 - val_loss: 2.6458\n",
            "Epoch 7/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.3368 - loss: 2.4524 - val_accuracy: 0.1111 - val_loss: 2.6537\n",
            "Epoch 8/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.2449 - loss: 2.4275 - val_accuracy: 0.1111 - val_loss: 2.6490\n",
            "Epoch 9/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.2449 - loss: 2.4121 - val_accuracy: 0.1111 - val_loss: 2.6245\n",
            "Epoch 10/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.2449 - loss: 2.3834 - val_accuracy: 0.2222 - val_loss: 2.6127\n",
            "Epoch 11/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.2958 - loss: 2.3284 - val_accuracy: 0.1111 - val_loss: 2.6287\n",
            "Epoch 12/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.2345 - loss: 2.2919 - val_accuracy: 0.1111 - val_loss: 2.6305\n",
            "Epoch 13/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.3264 - loss: 2.2625 - val_accuracy: 0.1111 - val_loss: 2.6235\n",
            "Epoch 14/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.2449 - loss: 2.2273 - val_accuracy: 0.1111 - val_loss: 2.6487\n",
            "Epoch 15/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.2345 - loss: 2.2247 - val_accuracy: 0.1111 - val_loss: 2.6981\n",
            "Epoch 16/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2449 - loss: 2.2085 - val_accuracy: 0.1111 - val_loss: 2.7389\n",
            "Epoch 17/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2449 - loss: 2.1820 - val_accuracy: 0.1111 - val_loss: 2.6724\n",
            "Epoch 18/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2449 - loss: 2.1296 - val_accuracy: 0.1111 - val_loss: 2.5874\n",
            "Epoch 19/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3368 - loss: 2.1303 - val_accuracy: 0.1111 - val_loss: 2.4887\n",
            "Epoch 20/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2449 - loss: 2.0961 - val_accuracy: 0.1111 - val_loss: 2.3802\n",
            "Epoch 21/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2449 - loss: 2.0069 - val_accuracy: 0.2222 - val_loss: 2.2676\n",
            "Epoch 22/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.3062 - loss: 1.9652 - val_accuracy: 0.3333 - val_loss: 2.1556\n",
            "Epoch 23/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4593 - loss: 1.9086 - val_accuracy: 0.3333 - val_loss: 2.0649\n",
            "Epoch 24/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3876 - loss: 1.8703 - val_accuracy: 0.4444 - val_loss: 1.9761\n",
            "Epoch 25/25\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5205 - loss: 1.8298 - val_accuracy: 0.4444 - val_loss: 1.9011\n",
            "\n",
            "Evaluating model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.7986\n",
            "Test Accuracy: 0.5000\n",
            "\n",
            "Model saved to hand_sign_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2) Settings — change these as needed\n",
        "CSV_PATH = \"/content/drive/MyDrive/labels.csv\"        # <-- your CSV path\n",
        "OUT_DIR  = \"/content/drive/MyDrive/label_clips\"       # <-- where clips go\n",
        "TARGET_LABEL = \"SEE_YOU_LATER FATHER\"                                # <-- choose a label\n",
        "\n",
        "# 3) Dependencies & imports\n",
        "import os, subprocess, shlex, pathlib\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure output folder exists\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# 4) Load CSV (expects columns: video_path,start,end,label)\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df.columns = [c.strip().lower() for c in df.columns]\n",
        "required = {\"video_path\",\"start\",\"end\",\"label\"}\n",
        "missing = required - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"CSV missing columns: {missing}\")\n",
        "\n",
        "# Drop incomplete rows\n",
        "df = df.dropna(subset=[\"video_path\",\"start\",\"end\",\"label\"])\n",
        "df[\"video_path\"] = df[\"video_path\"].astype(str).str.strip()\n",
        "df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
        "df[\"start\"] = df[\"start\"].astype(str).str.strip()\n",
        "df[\"end\"]   = df[\"end\"].astype(str).str.strip()\n",
        "\n",
        "# 5) Small helper: if path has no .mp4 and file doesn’t exist, try +\".mp4\"\n",
        "def resolve_video_path(p):\n",
        "    if os.path.exists(p):\n",
        "        return p\n",
        "    if not p.lower().endswith(\".mp4\") and os.path.exists(p + \".mp4\"):\n",
        "        return p + \".mp4\"\n",
        "    return p  # return original (ffmpeg will fail if truly missing)\n",
        "\n",
        "# 6) Clip function (re-encodes for accurate cuts)\n",
        "def clip_row(row, idx=None):\n",
        "    src = resolve_video_path(row[\"video_path\"])\n",
        "    if not os.path.exists(src):\n",
        "        print(f\"SKIP (missing source): {src}\")\n",
        "        return None\n",
        "\n",
        "    label = row[\"label\"]\n",
        "    start = row[\"start\"]   # e.g., 00:01:46.0\n",
        "    end   = row[\"end\"]     # e.g., 00:01:47.3\n",
        "\n",
        "    # Safe filename parts\n",
        "    safe_label = \"\".join(c if c.isalnum() or c in (\"_\",\"-\") else \"_\" for c in label)\n",
        "    base = pathlib.Path(src).stem\n",
        "    idx_suffix = f\"__{idx:03d}\" if idx is not None else \"\"\n",
        "    out_name = f\"{safe_label}__{base}__{start.replace(':','-')}__to__{end.replace(':','-')}{idx_suffix}.mp4\"\n",
        "    out_path = os.path.join(OUT_DIR, out_name)\n",
        "\n",
        "    # ffmpeg command (accurate cuts by seeking after -i; re-encodes to h264/aac)\n",
        "    cmd = (\n",
        "        f\"ffmpeg -y -hide_banner -loglevel error \"\n",
        "        f\"-i {shlex.quote(src)} -ss {shlex.quote(start)} -to {shlex.quote(end)} \"\n",
        "        f\"-c:v libx264 -preset veryfast -crf 18 -c:a aac -movflags +faststart \"\n",
        "        f\"{shlex.quote(out_path)}\"\n",
        "    )\n",
        "    ret = subprocess.run(cmd, shell=True)\n",
        "    if ret.returncode == 0:\n",
        "        print(\"WROTE\", out_path)\n",
        "        return out_path\n",
        "    else:\n",
        "        print(\"FFMPEG FAILED:\", out_path)\n",
        "        return None\n",
        "\n",
        "# 7) Filter rows for the chosen label and export\n",
        "subset = df[df[\"label\"].str.casefold() == TARGET_LABEL.casefold()].reset_index(drop=True)\n",
        "if subset.empty:\n",
        "    print(f\"No rows found for label: {TARGET_LABEL}\")\n",
        "else:\n",
        "    for i, row in subset.iterrows():\n",
        "        clip_row(row, idx=i+1)\n",
        "\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfvii8EKCFtz",
        "outputId": "c1ea6b45-07ee-4880-caa6-d885dae52abe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "No rows found for label: SEE_YOU_LATER FATHER\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === SETTINGS ===\n",
        "CSV_PATH   = \"/content/drive/MyDrive/labels.csv\"\n",
        "OUT_DIR    = \"/content/drive/MyDrive/label_clips\"\n",
        "LABELS     = [\"MOTHER\", \"FATHER\"]   # <- multiple labels here\n",
        "MERGED_OUT = \"/content/drive/MyDrive/label_clips/MOTHER__FATHER__merged.mp4\"\n",
        "\n",
        "import os, subprocess, shlex, pathlib\n",
        "import pandas as pd\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- Load & filter rows for the labels ---\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df.columns = [c.strip().lower() for c in df.columns]\n",
        "df = df.dropna(subset=[\"video_path\",\"start\",\"end\",\"label\"])\n",
        "\n",
        "labels_lower = [l.casefold() for l in LABELS]\n",
        "subset = df[df[\"label\"].astype(str).str.casefold().isin(labels_lower)].copy()\n",
        "\n",
        "# sort by start time so the merged video plays in order\n",
        "def t2s(t):\n",
        "    hh, mm, ss = t.split(\":\")\n",
        "    return int(hh)*3600 + int(mm)*60 + float(ss)\n",
        "subset[\"start_s\"] = subset[\"start\"].astype(str).map(t2s)\n",
        "subset = subset.sort_values([\"start_s\"]).reset_index(drop=True)\n",
        "\n",
        "def resolve_video_path(p):\n",
        "    p = str(p).strip()\n",
        "    if os.path.exists(p): return p\n",
        "    if not p.lower().endswith(\".mp4\") and os.path.exists(p + \".mp4\"):\n",
        "        return p + \".mp4\"\n",
        "    return p\n",
        "\n",
        "def safe_name(s):\n",
        "    return \"\".join(c if c.isalnum() or c in (\"_\",\"-\") else \"_\" for c in str(s))\n",
        "\n",
        "def clip_row(row, idx=None):\n",
        "    src = resolve_video_path(row[\"video_path\"])\n",
        "    if not os.path.exists(src):\n",
        "        print(\"SKIP (missing source):\", src); return None\n",
        "    start, end, label = str(row[\"start\"]), str(row[\"end\"]), str(row[\"label\"])\n",
        "    base = pathlib.Path(src).stem\n",
        "    idx_suffix = f\"__{idx:03d}\" if idx else \"\"\n",
        "    out_name = f\"{safe_name(label)}__{base}__{start.replace(':','-')}__to__{end.replace(':','-')}{idx_suffix}.mp4\"\n",
        "    out_path = os.path.join(OUT_DIR, out_name)\n",
        "\n",
        "    # Export the segment (re-encodes for accurate cuts)\n",
        "    cmd = (\n",
        "        f\"ffmpeg -y -hide_banner -loglevel error \"\n",
        "        f\"-i {shlex.quote(src)} -ss {shlex.quote(start)} -to {shlex.quote(end)} \"\n",
        "        f\"-c:v libx264 -preset veryfast -crf 18 -c:a aac -movflags +faststart \"\n",
        "        f\"{shlex.quote(out_path)}\"\n",
        "    )\n",
        "    ok = (subprocess.run(cmd, shell=True).returncode == 0)\n",
        "    return out_path if ok else None\n",
        "\n",
        "# --- ensure segments exist (creates them now) ---\n",
        "clip_paths = []\n",
        "for i, row in subset.iterrows():\n",
        "    p = clip_row(row, idx=i+1)\n",
        "    if p: clip_paths.append(p)\n",
        "\n",
        "if not clip_paths:\n",
        "    raise SystemExit(\"No clips were created. Check labels & paths in the CSV.\")\n",
        "\n",
        "# --- build concat list file ---\n",
        "list_txt = os.path.join(OUT_DIR, \"_concat_list.txt\")\n",
        "with open(list_txt, \"w\") as f:\n",
        "    for p in clip_paths:\n",
        "        f.write(f\"file '{p}'\\n\")\n",
        "\n",
        "# --- merge: fast path (no re-encode); if it fails, fallback to safe re-encode ---\n",
        "cmd_fast = f\"ffmpeg -y -hide_banner -loglevel error -f concat -safe 0 -i {shlex.quote(list_txt)} -c copy {shlex.quote(MERGED_OUT)}\"\n",
        "ret = subprocess.run(cmd_fast, shell=True).returncode\n",
        "\n",
        "if ret != 0:\n",
        "    print(\"Fast concat failed; trying safe re-encode…\")\n",
        "    cmd_safe = (f\"ffmpeg -y -hide_banner -loglevel error -f concat -safe 0 \"\n",
        "                f\"-i {shlex.quote(list_txt)} -c:v libx264 -preset veryfast -crf 18 \"\n",
        "                f\"-c:a aac -movflags +faststart {shlex.quote(MERGED_OUT)}\")\n",
        "    ret = subprocess.run(cmd_safe, shell=True).returncode\n",
        "\n",
        "print(\"MERGED:\", MERGED_OUT if ret == 0 else \"FAILED\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8FErEBmCFrv",
        "outputId": "e4b442c4-fe75-4805-a5cf-3923f3256199"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MERGED: /content/drive/MyDrive/label_clips/MOTHER__FATHER__merged.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8xoKTJ1BCFnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pZP_WftXCFgy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}